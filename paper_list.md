# Embodied-AI-Paper-TopConf
ðŸ”¥ NeuIPS2025 &amp; CORL2025 &amp; ICCV2025 &amp; ICML2025 &amp; RSS2025 &amp; CVPR2025 &amp; ICLR2025 Embodied AI Paper List  Resources.

[03/22/2025] We plan to organize more papers on Embodied AI from top conferences in the future and build a more comprehensive paper list. If there are any conference papers you would like to browse or if you have any other suggestions, please feel free to leave an issue.

[04/12/2025] We are updating Embodied AI papers accepted by RSS2025 (Robotics Top Conference)!

[05/21/2025] We are updating Embodied AI papers accepted by ICML2025!

[08/05/2025] We are updating Embodied AI papers accepted by ICCV2025!

[09/30/2025] We are updating Embodied AI papers accepted by CORL2025!

[11/30/2025] We are updating Embodied AI papers accepted by NeuIPS2025!

## ðŸ“– Paper List

- [ðŸ“– NeuIPS2025](#neuips2025)
  - [Vision-Language-Action Model](#vision-language-action-model)
  - [Data](#data)
  - [World Model](#world-model)
  - [Planning and Reasoning](#planning-and-reasoning)
  - [Navigation](#navigation)
  - [Humanoid](#humanoid)
  - [3D Vision](#3d-vision)
  - [Policy](#policy)
  - [Accelerating and Deploying](#accelerating-and-deploying)
  - [Tactile](#tactile)
  - [Dexterous](#dexterous)
  - [Benchmark and Dataset](##benchmark-and-dataset)
- [ðŸ“– CORL2025](#corl2025)
  - [Vision-Language-Action Model](#vision-language-action-model)
  - [World Model](#world-model)
  - [Policy](#policy)
  - [Humanoid](#humanoid)
  - [Navigation](#navigation)
  - [Benchmark and Dataset](#benchmark-and-dataset)
  - [Dexterous Manipulation](dexterous-manipulation)
  - [Sim-to-Real](#sim-to-real)
- [ðŸ“– ICCV2025](#iccv2025)
  - [Vision-Language-Action Model](#vision-language-action-model)
  - [Vision-Language-Navigation Model](#vision-language-navigation-model)
  - [Hierarchical Planning](#hierarchical-planning)
  - [World Model](#world-model)
  - [Policy](#policy)
  - [Accelerating and Deploying](#accelerating-and-deploying)
  - [Perception](#perception)
  - [Benchmark and Dataset](#benchmark-and-dataset)
- [ðŸ“– ICML2025](#icml2025)
  - [Vision-Language-Action Models](#vision-language-action-models)
  - [Planning and Reasoning](#planning-and-reasoning)
  - [Policies](#policies)
  - [3D Vision](#3d-vision)
  - [Dataset](#dataset)
- [ðŸ“– RSS2025](#rss2025)
- [ðŸ“– CVPR2025](#cvpr2025)
  - [Vision-Language-Action Models](#vision-language-action-models)
  - [Policies](#policies)
  - [Grasp](#grasp)
  - [Humanoid](#humanoid)
  - [Planning and Reasoning](#planning-and-reasoning)
  - [3D Vision](#3d-vision)
  - [Sim2real and Real2sim](#sim2real-and-real2sim)
  - [Benchmark and Dataset](#benchmark-and-dataset)
- [ðŸ“– ICLR2025](#iclr2025)
  - [Vision-Language-Action Models](#vision-language-action-models)
  - [Policies](#policies)
  - [Planning and Reasoning](#planning-and-reasoning)
  - [3D Vision](#3d-vision)
  - [Sim2real and Real2sim](#sim2real-and-real2sim)
- [ðŸ“– ICRA2025](#icra2025)

# NeuIPS2025
## Vision-Language-Action Model
- **Fast-in-Slow**: A Dual-System VLA Model Unifying Fast Manipulation within Slow Reasoning [Paper](https://arxiv.org/abs/2506.01953) [Page](https://fast-in-slow.github.io/)
- **AC-DiT**: Adaptive Coordination Diffusion Transformer for Mobile Manipulation [Paper](https://arxiv.org/abs/2507.01961) [Page](https://ac-dit.github.io/)
- **BridgeVLA**: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models [Paper](https://arxiv.org/abs/2506.07961) [Page](https://bridgevla.github.io/)
- **CogVLA**: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification [Paper](https://arxiv.org/abs/2508.21046) [Page](https://jiutian-vl.github.io/CogVLA-page/)
- **VideoVLA**: Video Generators Can Be Generalizable Robot Manipulators
- **ChatVLA-2**: Vision-Language-Action Model with Open-World Reasoning [Paper](https://arxiv.org/abs/2505.21906) [Page](https://chatvla-2.github.io/)
- Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization [Paper](https://arxiv.org/abs/2505.15660) [Page](https://jiaming-zhou.github.io/AGNOSTOS/)
- **BadVLA**: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization [Paper](https://arxiv.org/abs/2505.16640) [Page](https://github.com/Zxy-MLlab/BadVLA)
- **Compliant Residual DAgger**: Improving Real-World Contact-Rich Manipulation with Human Corrections [Paper](https://arxiv.org/abs/2506.16685) [Page](https://compliant-residual-dagger.github.io/)
- **VLA-OS**: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models [Paper](https://arxiv.org/abs/2506.17561) [Page](https://nus-lins-lab.github.io/vlaos/)
- **ThinkAct**: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning [Paper](https://arxiv.org/abs/2507.16815) [Page](https://jasper0314-huang.github.io/thinkact-vla/)
- Self-Improving Embodied Foundation Models [Paper](https://arxiv.org/abs/2509.15155) [Page](https://self-improving-efms.github.io/)
- **Robo2VLM**: Improving Visual Question Answering using Large-Scale Robot Manipulation Data [Paper](https://arxiv.org/abs/2505.15517) [Page](https://berkeleyautomation.github.io/robo2vlm/)
- **EnerVerse**: Envisioning Embodied Future Space for Robotics Manipulation [Paper](https://arxiv.org/abs/2501.01895)
- Learning Spatial-Aware Manipulation Ordering [Paper](https://arxiv.org/abs/2510.25138)
- **PRIMT**: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models [Paper](https://arxiv.org/abs/2509.15607)
- **BEAST**: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning [Paper](https://arxiv.org/abs/2506.06072)
- **PointMapPolicy**: Structured Point Cloud Processing for Multi-Modal Imitation Learning [Paper](https://arxiv.org/abs/2510.20406)
- Real-Time Execution of Action Chunking Flow Policies [Paper](https://www.physicalintelligence.company/download/real_time_chunking.pdf) [Page](https://www.physicalintelligence.company/research/real_time_chunking)
- **Chain-of-Action**: Trajectory Autoregressive Modeling for Robotic Manipulation [Paper](https://arxiv.org/abs/2506.09990) [Page](https://chain-of-action.github.io/)
- **4D-VLA**: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration
- **SAFE**: Multitask Failure Detection for Vision-Language-Action Models [Paper](https://arxiv.org/abs/2506.09937) [Page](https://vla-safe.github.io/)
- Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames [Paper](https://arxiv.org/abs/2510.24194) [Page](https://sites.google.com/view/blindfoldedexperts/home)
- **HiMaCon:** Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data [Paper](https://arxiv.org/abs/2510.11321)
- Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better [Paper](https://arxiv.org/abs/2505.23705)
- Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents [Paper](https://arxiv.org/abs/2502.01218) [Page](https://actol-pretrain.github.io/)
- **DreamVLA**: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge [Paper](https://arxiv.org/abs/2507.04447) [Page](https://zhangwenyao1.github.io/DreamVLA/index.html)

## Data
- **EgoBridge**: Domain Adaptation for Generalizable Imitation from Egocentric Human Data [Paper](https://arxiv.org/abs/2509.19626) [Page](https://ego-bridge.github.io/)
- **RobotSmith**: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skill [Paper](https://arxiv.org/abs/2506.14763) [Page](https://umass-embodied-agi.github.io/RobotSmith/)
- **URDF-Anything**: Constructing Articulated Objects with 3D Multimodal Language Model [Paper](https://arxiv.org/abs/2511.00940)
- **DEAL**: Diffusion Evolution Adversarial Learning for Sim-to-Real Transfer
- Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training [Paper](https://arxiv.org/abs/2509.18631) [Page](https://ot-sim2real.github.io/)

## World Model
- **SAMPO**: Scale-wise Autoregression with Motion Prompt for Generative World Models [Paper](https://arxiv.org/abs/2509.15536)
- Learning 3D Persistent Embodied World Models [Paper](https://arxiv.org/abs/2505.05495)
- **OSVI-WM**: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation [Paper](https://arxiv.org/abs/2505.20425)

## Planning and Reasoning
- Towards Reliable LLM-based Robots Planning via Combined Uncertainty Estimation [Paper](https://arxiv.org/abs/2510.08044)
- **Towards Reliable Code-as-Policies**: A Neuro-Symbolic Framework for Embodied Task Planning [Paper](https://arxiv.org/abs/2510.21302)
- **RDD**: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks [Paper](https://arxiv.org/abs/2510.14968) [Page](https://rdd-neurips.github.io/)
- **UniDomain**: Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning [Paper](https://arxiv.org/abs/2507.21545)
- InstructFlow: Adaptive Symbolic Constraint-Guided Code Generation for Long-Horizon Planning

## Navigation
- **C-NAV**: Towards Self-Evolving Continual Object Navigation in Open World [Paper](https://arxiv.org/abs/2510.20685) [Page](https://bigtree765.github.io/C-Nav-project/)
- Distilling LLM Prior to Flow Model for Generalizable Agentâ€™s Imagination in Object Goal Navigation [Paper](https://arxiv.org/abs/2508.09423)
- **TP-MDDN**: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making
- Active Test-time Vision-Language Navigation [Paper](https://arxiv.org/abs/2506.06630)
- **Aux-Think**: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation  
- **EfficientNav**: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval [Paper](https://arxiv.org/abs/2510.18546)
- **Seeing through Uncertainty**: Robust Task-Oriented Optimization in Visual Navigation [Paper](https://arxiv.org/abs/2510.00441) [Page](https://github.com/PyyWill/NeuRO)

## Humanoid
- Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning [Paper](https://arxiv.org/abs/2504.14305) [Page](https://almi-humanoid.github.io/)
- From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots [Paper](https://arxiv.org/abs/2506.12779) [Page](https://beingbeyond.github.io/BumbleBee/)
- **KungfuBot**: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills [Paper](https://kungfu-bot.github.io/) [Page](https://arxiv.org/abs/2506.12851)

## 3D Vision
- **DynaRend**: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation [Paper](https://arxiv.org/abs/2510.24261)
- Building 3D Representations and Generating Motions From a Single Image via Video-Generation [Paper](https://neurips.cc/virtual/2025/loc/san-diego/poster/118141)

## Policy
- Emerging Risks from Embodied AI Require Urgent Policy Action
- Human-assisted Robotic Policy Refinement via Action Preference Optimization [Paper](https://arxiv.org/abs/2506.07127) [Page](https://gewu-lab.github.io/action_preference_optimization/)
- *Hyper-GoalNet*: Goal-Conditioned Manipulation Policy Learning with HyperNetworks
- **ReinFlow**: Fine-tuning Flow Matching Policy with Online Reinforcement Learning [Paper](https://arxiv.org/abs/2505.22094) [Page](https://reinflow.github.io/)
- Diversifying Parallel Ergodic Search: A Signature Kernel Evolution Strategy
- **FreqPolicy**: Efficient Flow-based Visuomotor Policy via Frequency Consistency [Paper](https://arxiv.org/abs/2506.08822)
- A Practical Guide for Incorporating Symmetry in Diffusion Policy [Paper](https://arxiv.org/abs/2505.13431)
- **Latent Policy Barrier**: Learning Robust Visuomotor Policies by Staying In-Distribution [Paper](https://arxiv.org/abs/2508.05941) [Page](https://project-latentpolicybarrier.github.io/)
- Quantization-Free Autoregressive Action Transformer [Paper](https://arxiv.org/abs/2503.14259)
- Real-World Reinforcement Learning of Active Perception Behaviors
- Failure Prediction at Runtime for Generative Robot Policies [Paper](https://arxiv.org/abs/2510.09459)
- **Act to See, See to Act**: Diffusion-Driven Perception-Action Interplay for Adaptive Policies [Paper](https://arxiv.org/abs/2509.25822) [Page](https://jingwang18.github.io/dp-ag.github.io/)
- **Dynamic Test-Time Compute Scaling in Control Policy**: Difficulty-Aware Stochastic Interpolant Policy [Paper](https://arxiv.org/abs/2511.20906)
- **DynaGuide**: Steering Diffusion Polices with Active Dynamic Guidance [Paper](https://arxiv.org/abs/2506.13922) [Page](https://dynaguide.github.io/)
- World-aware Planning Narratives Enhance Large Vision-Language Model Planner [Paper](https://arxiv.org/abs/2506.21230)
 
## Accelerating and Deploying
- Accelerating Visual-Policy Learning through Parallel Differentiable Simulation [Paper](https://www.arxiv.org/abs/2505.10646) [Page](https://haoxiangyou.github.io/Dva_website/)
- **EfficientVLA**: Training-Free Acceleration and Compression for Vision-Language-Action Models [Paper](https://arxiv.org/abs/2506.10100)
- A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via Learning to Search [Paper](https://arxiv.org/abs/2506.05294) [Page](https://gokul.dev/sailor/)
- **VLA-Cache**: Efficient Vision-Language-Action Manipulation via Adaptive Token Caching [Paper](https://arxiv.org/abs/2502.02175) [Page](https://vla-cache.github.io/)

## Tactile
- Universal Visuo-Tactile Video Understanding for Embodied Interaction [Paper](https://arxiv.org/abs/2505.22566)
- Enhancing Tactile-based Reinforcement Learning for Robotic Control [Paper](https://arxiv.org/abs/2510.21609) [Page](https://elle-miller.github.io/tactile_rl/)
- **Taccel**: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation [Paper](https://taccel-simulator.github.io/assets/taccel-paper.pdf) [Page](http://taccel-simulator.github.io/)
- **Toward Artificial Palpation**: Representation Learning of Touch on Soft Bodies [Paper](https://arxiv.org/abs/2511.16596) [Page](https://zoharri.github.io/artificial-palpation/)
- **Touch in the Wild**: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper [Paper](https://arxiv.org/abs/2507.15062v1) [Page](https://binghao-huang.github.io/touch_in_the_wild/)

## Dexterous
- Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation [Paper](https://arxiv.org/pdf/2511.01276) [Page](https://cmtdiffusion.github.io/)
- **HumanoidGen**: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning [Paper](https://arxiv.org/abs/2507.00833) [Page](https://arxiv.org/abs/2507.00833)
- **Grasp2Grasp**: Vision-Based Dexterous Grasp Translation via SchrÃ¶dinger Bridges [Paper](https://arxiv.org/abs/2506.02489) [Page](https://grasp2grasp.github.io/)
- Scaffolding Dexterous Manipulation with Vision-Language Models [Paper](https://arxiv.org/abs/2506.19212) [Page](https://sites.google.com/view/dexterous-vlm-scaffolding)
- **DexFlyWheel**: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation [Paper](https://arxiv.org/abs/2509.23829) [Page](https://dexflywheel.github.io/)
- **DexGarmentLab**: Dexterous Garment Manipulation Environment with Generalizable Policy [Paper](https://arxiv.org/abs/2505.11032) [Page](https://wayrise.github.io/DexGarmentLab/)

## Benchmark and Dataset
- **RoboCerebra**: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation [Paper](https://www.arxiv.org/pdf/2506.06677) [Page](https://github.com/qiuboxiang/RoboCerebra)
- **SutureBot**: A Precision Framework & Benchmark For Autonomous End-to-End Suturing [Paper](https://suturebot.github.io/static/SutureBot_NeurIPS_2025.pdf) [Page](https://suturebot.github.io/)
- Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration
- **LabUtopia**: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents [Paper](https://arxiv.org/abs/2505.22634) [Page](https://rui-li023.github.io/labutopia-site/)
- **SonoGym**: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound [Paper](https://arxiv.org/abs/2507.01152) [Page](https://github.com/SonoGym/SonoGym)
- Embodied Crowd Counting
- **PAC Bench**: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies? [Paper](https://arxiv.org/abs/2506.23725)

# CORL2025

## Vision-Language-Action Model

- **$\pi_{0.5}$**: a Vision-Language-Action Model with Open-World Generalization [Paper](https://arxiv.org/abs/2504.16054) [page](https://www.pi.website/blog/pi05)
- Training Strategies for Efficient Embodied Reasoning[Paper](https://arxiv.org/abs/2505.08243) [page](https://ecot-lite.github.io/)
- **Long-VLA**: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation [Paper](https://arxiv.org/abs/2508.19958) [page](https://long-vla.github.io/)
- **RoboMonkey**: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models [Paper](https://arxiv.org/abs/2506.17811) [page](https://robomonkey-vla.github.io/)
- **RoboChemist**: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation [Paper](https://arxiv.org/pdf/2509.08820) [page](https://zzongzheng0918.github.io/RoboChemist.github.io/)
- **TA-VLA**: Elucidating the Design Space of Torque-aware Vision-Language-Action Models [Paper](https://arxiv.org/abs/2509.07962) [page](https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/)
- **Focusing on What Matters**: Object-Agent-centric Tokenization for Vision Language Action models [Paper](https://openreview.net/forum?id=Ict1OjU9gl#discussion) 
- **FLOWER**: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies [Paper](https://arxiv.org/abs/2509.04996) [page](https://intuitive-robots.github.io/flower_vla/)
- Mechanistic Interpretability for Steering Vision-Language-Action Models [Paper](https://arxiv.org/pdf/2509.00328) 
- **RICL**: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models [Paper](https://arxiv.org/abs/2508.02062) [page](https://ricl-vla.github.io/)
- **DexVLA**: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control [Paper](https://arxiv.org/abs/2502.05855) [page](https://github.com/juruobenruo/DexVLA)
- **FLARE**: Robot Learning with Implicit World Modeling [Paper](https://arxiv.org/abs/2505.15659) [page](https://research.nvidia.com/labs/gear/flare/)
- **3DS-VLA**: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation [Paper](https://openreview.net/forum?id=dT45OMevL5#discussion) 
- **GraspVLA**: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data [Paper](https://arxiv.org/abs/2505.03233) [page](https://pku-epic.github.io/GraspVLA-web/)
- **EndoVLA**: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy [Paper](https://arxiv.org/abs/2505.15206) 
- **MoTo**: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation [Paper](https://arxiv.org/abs/2509.01658) [page](https://gary3410.github.io/MoTo/)
- **ControlVLA**: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models [Paper](https://arxiv.org/pdf/2506.16211) [page](https://controlvla.github.io/)
- **TrackVLA**: Embodied Visual Tracking in the Wild [Paper](https://arxiv.org/abs/2505.23189) [page](https://pku-epic.github.io/TrackVLA-web/)
- **AnyPlace**: Learning Generalizable Object Placement for Robot Manipulation [Paper](https://arxiv.org/abs/2502.04531) [page](https://any-place.github.io/)
- Generalist Robot Manipulation beyond Action Labeled Data [Paper](https://arxiv.org/pdf/2509.19958) [page](https://motovla.github.io/)
- **LaVA-Man**: Learning Visual Action Representations for Robot Manipulation [Paper](https://arxiv.org/abs/2508.19391) [page](https://qm-ipalab.github.io/LaVA-Man/)

## Navigation

- **MoTo**: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation 
- Meta-Optimization and Program Search using Language Models for Task and Motion Planning 
- **ObjectReact**: Learning Object-Relative Control for Visual Navigation
- **HALO**: Human Preference Aligned Offline Reward Learning for Robot Navigation
- Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models
- **Long Range Navigator (LRN)**: Extending robot planning horizons beyond metric maps
- **Search-TTA**: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild
- **ActLoc**: Learning to Localize on the Move via Active Viewpoint Selection
- Human-like Navigation in a World Built for Humans
- **GC-VLN**: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation
- **GraspMolmo**: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation
- **Belief-Conditioned One-Step Diffusion**: Real-Time Trajectory Planning with Just-Enough Sensing
## Policy

- **ImMimic**: Cross-Domain Imitation from Human Videos via Mapping and Interpolation [Paper](https://arxiv.org/abs/2509.10952) [page](https://sites.google.com/view/immimic)
- **ReWiND**: Language-Guided Rewards Teach Robot Policies without New Demonstrations [Paper](https://arxiv.org/abs/2505.10911) [page](https://rewind-reward.github.io/)
- Steering Your Diffusion Policy with Latent Space Reinforcement Learning [Paper](https://arxiv.org/abs/2506.15799) [page](https://diffusion-steering.github.io/)
- **Streaming Flow Policy**: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories [Paper](https://arxiv.org/abs/2505.21851) [page](https://siddancha.github.io/streaming-flow-policy/)
- **SAIL**: Faster-than-Demonstration Execution of Imitation Learning Policies [Paper](https://arxiv.org/abs/2506.11948) [page](https://nadunranawaka1.github.io/sail-policy/)
- Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance [Paper](https://arxiv.org/abs/2509.03889) [page](https://mhtippur.github.io/inairclothmanipulation/)
- Data Retrieval with Importance Weights for Few-Shot Imitation Learning [Paper](https://arxiv.org/pdf/2509.01657) [page](https://rahulschand.github.io/iwr/)
- **X-Sim**: Cross-Embodiment Learning via Real-to-Sim-to-Real [Paper](https://arxiv.org/abs/2505.07096) 
- **DemoSpeedup**: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration [Paper](https://arxiv.org/abs/2506.05064) [page](https://demospeedup.github.io/)
- **ManiFlow**: A General Robot Manipulation Policy via Consistency Flow Training [Paper](https://arxiv.org/abs/2509.01819) [page](https://maniflow-policy.github.io/)
- **Text2Touch**: Tactile In-Hand Manipulation with LLM-Designed Reward Functions [Paper](https://arxiv.org/abs/2509.07445) [page](https://hpfield.github.io/text2touch-website/)
- **Multi-Loco**: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion [Paper](https://arxiv.org/abs/2506.11470) [page](https://mops-tamp.github.io/)
- $\texttt{SPIN}$: distilling $\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation [Paper](https://arxiv.org/abs/2502.18015) 
- Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics [Paper](https://arxiv.org/abs/2509.04737) 
- Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization [Paper](https://arxiv.org/pdf/2508.03944) [page](https://cp-gen.github.io/)
- **CLASS**: Contrastive Learning via Action Sequence Supervision for Robot Manipulation [Paper](https://arxiv.org/abs/2508.01600) [page](https://class-robot.github.io/)
- **MirrorDuo**: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs [page](https://github.com/zheyu-zhuang/mirror-duo?tab=readme-ov-file)
- Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation [Paper](https://arxiv.org/abs/2508.21375)
- **Eye, Robot**: Learning to Look to Act with a BC-RL Perception-Action Loop [Paper](https://arxiv.org/abs/2506.10968) [page](https://www.eyerobot.net/)
- **ARCH**: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly [Paper](https://arxiv.org/abs/2409.16451) [page](https://long-horizon-assembly.github.io/)
- **KDPE**: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection [Paper](https://arxiv.org/pdf/2508.10511) [page](https://hsp-iit.github.io/KDPE/)
- **AimBot**: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies [Paper](https://arxiv.org/abs/2508.08113) [page](https://aimbot-reticle.github.io/)
- Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions 
- **Mobi-$\pi$**: Mobilizing Your Robot Learning Policy [Paper](https://arxiv.org/abs/2505.23692) [page](https://mobipi.github.io/)
- Action-Free Reasoning for Policy Generalization [Paper](https://arxiv.org/abs/2502.03729) [page](https://rad-generalization.github.io/)
- **Learn from What We HAVE**: History-Aware VErifier that Reasons about Past Interactions Online [Paper](https://arxiv.org/abs/2509.00271v1) [page](https://liy1shu.github.io/HAVE_CoRL25/)
- **D-CODA**: Diffusion for Coordinated Dual-Arm Data Augmentation [Paper](https://arxiv.org/abs/2505.04860) [page](https://dcodaaug.github.io/D-CODA/)
- **ATK**: Automatic Task-driven Keypoint Selection for Robust Policy Learning [Paper](https://arxiv.org/abs/2506.13867) [page](https://yunchuzhang.github.io/ATK/)
- **Poke and Strike**: Learning Task-Informed Exploration Policies [Paper](https://arxiv.org/abs/2509.00178) [page](https://marina-aoyama.github.io/poke-and-strike/)
- **SafeBimanual**: Diffusion-based trajectory optimization for safe bimanual manipulation [Paper](https://arxiv.org/abs/2508.18268) [page](https://denghaoyuan123.github.io/SafeBimanip/)
- **COMBO-Grasp**: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping [Paper](https://arxiv.org/abs/2502.08054) 
- **Phantom**: Training Robots Without Robots Using Only Human Videos [Paper](https://arxiv.org/abs/2503.00779) [page](https://phantom-human-videos.github.io/)
- Learning Long-Context Diffusion Policies via Past-Token Prediction [Paper](https://openreview.net/forum?id=N4WWF8Les5) 
- **VT-Refine**: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning [Paper](https://openreview.net/forum?id=mV3W5givYb) 
- **COLLAGE**: Adaptive Fusion-based Retrieval for Augmented Policy Learning [Paper](https://arxiv.org/abs/2508.01131) [page](https://robin-lab.cs.utexas.edu/COLLAGE/)
- **CDP**: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion [Paper](https://arxiv.org/abs/2506.14769) [page](https://gaavama.github.io/CDP/)
- Robust Dexterous Grasping of General Objects [Paper](https://arxiv.org/abs/2504.05287) [page](https://zdchan.github.io/Robust_DexGrasp/)
- **Point Policy**: Unifying Observations and Actions with Key Points for Robot Manipulation [Paper](https://arxiv.org/abs/2502.20391) [page](https://point-policy.github.io/)
## Benchmark and Dataset

- **RoboArena**: Distributed Real-World Evaluation of Generalist Robot Policies
- **GraspVLA**: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data 
- **CUPID**: Curating Data your Robot Loves with Influence 
- **AutoEval**: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World
- **ManipBench**: Benchmarking Vision-Language Models for Low-Level Robot Manipulation Functions
- Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration
- Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration
- **UniSkill**: Imitating Human Videos via Cross-Embodiment Skill Representations
## Humanoid

- **HuB**: Learning Extreme Humanoid Balance
- Versatile Loco-Manipulation through Flexible Interlimb Coordination
- Visual Imitation Enables Contextual Humanoid Control
- **Hand-Eye Autonomous Delivery**: Learning Humanoid Navigation, Locomotion and Reaching
- **CLONE**: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks 
- **Embrace Contacts**: humanoid shadowing with full body ground contacts
- **Hold My Beer**: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control
- **SLAC**: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL
- **Robot Trains Robot**: Automatic Real-World Policy Adaptation and Learning for Humanoids
- Humanoid Policy ~ Human Policy
## World Model

- **Real2Render2Real**: Scaling Robot Data Without Dynamics Simulation or Robot Hardware
- Cross-Sensor Touch Generation
- **WoMAP**: World Models For Embodied Open-Vocabulary Object Localization
- **DreamGen**: Unlocking Generalization in Robot Learning through Video World Models
- **Tool-as-Interface**: Learning Robot Policies from Observing Human Tool Use
- Articulated Object Estimation in the Wild
- **DiWA**: Diffusion Policy Adaptation with World Models
- Steerable Scene Generation with Post Training and Inference-Time Search
- Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation
- **Gen2Act**: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation
- **Reflective Planning**: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation
- **LaDi-WM**: A Latent Diffusion-Based World Model for Predictive Manipulation
## Dexterous Manipulation

- **DexUMI**: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation [page](https://dex-umi.github.io/)
- **Dexplore**: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration
- **FFHFlow**: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference
- **GraspQP**: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping [page](https://graspqp.github.io/)
- Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation
- **KineDex**: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation
- **D-Cubed**: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation
- **LodeStar**: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations 
## Sim-to-Real

- **The Sound of Simulation**: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio
- **FetchBot**: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real
- **ClutterDexGrasp**: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes
- **SimShear**: Sim-to-Real Shear-based Tactile Servoing
- **Wheeled Lab**: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics
- Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling
- **AgentWorld**: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation
- Robot Learning from Any Images
# ICCV2025

## Vision-Language-Action Model
- Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics [Paper](https://arxiv.org/abs/2411.13587) [page](https://vlaattacker.github.io/)
- **VQ-VLA**: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers [Paper](https://arxiv.org/abs/2507.01016) [page](https://xiaoxiao0406.github.io/vqvla.github.io)
- **Dita**: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy [Paper](https://arxiv.org/abs/2503.19757) [page](https://robodita.github.io/)
- **Moto**: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos [Paper](https://arxiv.org/abs/2412.04445) [page](https://chenyi99.github.io/moto/)
- **A0**: An Affordance-Aware Hierarchical Model for General Robotic Manipulation [Paper](https://arxiv.org/abs/2504.12636) [page](https://a-embodied.github.io/A0/)
- **Embodied VideoAgent**: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding [Paper](https://arxiv.org/abs/2501.00358) [page](https://embodied-videoagent.github.io/)
- **CoA-VLA**: Improving Vision-Language-Action Models via Visual-Text Chain-of-Affordance [Paper](https://iccv.thecvf.com/virtual/2025/poster/542)
- **FedVLA**: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation [Paper](https://iccv.thecvf.com/virtual/2025/poster/1325)
- Towards Long-Horizon Vision-Language-Action System: Reasoning, Acting and Memory [Paper](https://iccv.thecvf.com/virtual/2025/poster/1915)
- **PASG**: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation [Paper](https://iccv.thecvf.com/virtual/2025/poster/225)
- **SD2Actor**: Continuous State Decomposition via Diffusion Embeddings for Robotic Manipulation [Paper](https://iccv.thecvf.com/virtual/2025/poster/1571)

## Vision-Language-Navigation Model
- **Move to Understand a 3D Scene**: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation [Paper](https://arxiv.org/abs/2507.04047) [page](https://mtu3d.github.io/)
- Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities [Paper](https://arxiv.org/abs/2507.13019) [page](https://crystalsixone.github.io/vln_pe.github.io/)
- **P3Nav**: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction [Paper](https://arxiv.org/abs/2503.18525)
- **SAME**: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts [Paper](https://arxiv.org/abs/2412.05552) [page](https://github.com/GengzeZhou/SAME)
- **NavMorph**: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments  [Paper](https://arxiv.org/abs/2506.23468) [page](https://github.com/Feliciaxyao/NavMorph)
- Harnessing Input-adaptive Inference for Efficient VLN [Paper](https://openreview.net/pdf?id=5gptKWnVPF)
- Embodied Navigation with Auxiliary Task of Action Description Prediction [Paper](https://iccv.thecvf.com/virtual/2025/poster/1984)
- 3D Gaussian Map with Open-Set Semantic Grouping for Vision-Language Navigation [Paper](https://iccv.thecvf.com/virtual/2025/poster/299)
- **NavQ**: Learning a Q-Model for Foresighted Vision-and-Language Navigation [Paper](https://iccv.thecvf.com/virtual/2025/poster/944)
- **monoVLN**: Bridging the Observation Gap between Monocular and Panoramic Vision and Language Navigation [Paper](https://iccv.thecvf.com/virtual/2025/poster/1792)

## Hierarchical Planning
- Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding [Paper](https://arxiv.org/abs/2507.18276)
- **CogNav**: Cognitive Process Modeling for Object Goal Navigation with LLMs [Paper](https://arxiv.org/abs/2412.10439) [page](https://yhancao.github.io/CogNav/)
- **RoBridge**: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation [Paper](https://arxiv.org/abs/2505.01709) [page](https://abliao.github.io/RoBridge/)

## World Model
- **IRASim**: A Fine-Grained World Model for Robot Manipulation [Paper](https://arxiv.org/abs/2406.14540) [page](https://gen-irasim.github.io/)
- **GWM**: Towards Scalable Gaussian World Models for Robotic Manipulation [Paper](https://ziweiwangthu.github.io/data/GWM.pdf) [page](https://gaussian-world-model.github.io/)
- **DyWA**: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation [Paper](https://arxiv.org/abs/2503.16806) [page](https://pku-epic.github.io/DyWA/)
- Diffusion-Based Imaginative Coordination for Bimanual Manipulation [Paper](https://arxiv.org/abs/2507.11296)
- Learning 4D Embodied World Models [Paper](https://openreview.net/pdf?id=mnwlhvmKMN)

## Policy
- Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework [Paper](https://arxiv.org/abs/2503.09186)
- **EC-Flow**: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow [Paper](https://arxiv.org/abs/2507.06224) [page](https://ec-flow1.github.io/)
- **Dense Policy**: Bidirectional Autoregressive Learning of Actions [Paper](https://arxiv.org/abs/2503.13217) [page](https://selen-suyue.github.io/DspNet/)
- **AnyBimanual**: Transferring Unimanual Policy for General Bimanual Manipulation [Paper](https://arxiv.org/abs/2412.06779) [page](https://anybimanual.github.io/)
- Learning Precise Affordances from Egocentric Videos for Robotic Manipulation [Paper](https://arxiv.org/abs/2408.10123v1) [page](https://reagan1311.github.io/affgrasp)
- **iManip**: Skill-Incremental Learning for Robotic Manipulation [Paper](https://arxiv.org/abs/2503.07087) 
- Spatial-Temporal Aware Visuomotor Diffusion Policy Learning [Paper](https://arxiv.org/abs/2507.06710) [page](https://zhenyangliu.github.io/DP4/)
- **Wavelet Policy**: Lifting Scheme for Policy Learning in Long-Horizon Tasks [Paper](https://arxiv.org/abs/2507.04331) [page](https://hhuang-code.github.io/wavelet_policy/)
- 4D Visual Pre-training for Robot Learning [Paper](https://iccv.thecvf.com/virtual/2025/poster/972)

## Accelerating and Deploying
- Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control [Paper](https://arxiv.org/abs/2505.15304)
- On-Device Diffusion Transformer Policy for Efficient Robot Manipulation [Paper](https://arxiv.org/abs/2508.00697)
- **COSMO**: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation [Paper](https://arxiv.org/abs/2503.24065)
- **CARP**: Coarse-to-Fine Autoregressive Prediction for Visuomotor Policy Learning [Paper](https://arxiv.org/abs/2412.06782) [page](https://carp-robot.github.io/)

## Perception
- **EmbodiedOcc**: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding [Paper](https://arxiv.org/abs/2412.04380) [page](https://ykiwu.github.io/EmbodiedOcc/)
- **Embodied Image Captioning**: Self-supervised Learning Agents for Spatially Coherent Image Descriptions [Paper](https://arxiv.org/abs/2504.08531) [page](https://hsp-iit.github.io/embodied-captioning/)

## Benchmark and Dataset
- **VLABench**: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks [Paper](https://arxiv.org/abs/2412.18194) [page](https://iranqin.github.io/robofactory/)
- **RoboFactory**: Exploring Embodied Agent Collaboration with Compositional Constraints [Paper](https://arxiv.org/abs/2503.16408) [page](https://vlabench.github.io/)
- **HUMOTO**: A 4D Dataset of Mocap Human Object Interactions [Paper](https://arxiv.org/abs/2504.10414) [page](https://jiaxin-lu.github.io/humoto/)
- **RoboMM**: All-in-One Multimodal Large Model for Robotic Manipulation [Paper](https://arxiv.org/abs/2412.07215)
- **MoMa-Kitchen**: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation [Paper](https://arxiv.org/abs/2503.11081) [page](https://momakitchen.github.io/)
- **RoboPearls**: Editable Video Simulation for Robot Manipulation [Paper](https://arxiv.org/abs/2506.22756) [page](https://tangtaogo.github.io/RoboPearls/)
- **DexH2R**: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover [Paper](https://arxiv.org/abs/2506.23152) [page](https://dexh2r.github.io/)
- **Beyond the Destination**: A Novel Benchmark for Exploration-Aware Embodied Question Answering [Paper](https://arxiv.org/abs/2503.11117) [page](https://github.com/HCPLab-SYSU/EXPRESS-Bench)
- **RobAVA**: A Large-scale Dataset and Baseline Towards Video based Robotic Arm Action Understanding [Paper](https://iccv.thecvf.com/virtual/2025/poster/1787)
- **RoboAnnotatorX**: A Comprehensive and Universal Annotation Framework for Accurate Understanding of Long-horizon Robot Demonstration [Paper](https://iccv.thecvf.com/virtual/2025/poster/2215)

# ICML2025

## Vision-Language-Action Models
- **Hi Robot**: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models [Paper](https://arxiv.org/abs/2502.19417)
- **OTTER**: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction [paper](https://arxiv.org/pdf/2503.03734) [page](https://ottervla.github.io/)
- **UP-VLA**: A Unified Understanding and Prediction Model for Embodied Agent [paper](https://arxiv.org/abs/2501.18867)
- **ELEMENTAL**: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics [paper](https://arxiv.org/abs/2411.18825)
- **ReinboT**: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning [paper](https://arxiv.org/abs/2505.07395)
- **A Large Recurrent Action Model:** xLSTM enables Fast Inference for Robotics Tasks [paper](https://arxiv.org/abs/2410.22391) [page](https://github.com/ml-jku/LRAM)

## Planning and Reasoning
- Efficient Robotic Policy Learning via Latent Space Backward Planning [paper](https://arxiv.org/abs/2505.06861) [page](https://lbp-authors.github.io/)
- Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling [paper](https://arxiv.org/pdf/2410.01440) [page](https://github.com/Singularity0104/equilibrium-planner)

## Policies

- **SAM2Act**:Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation [paper](https://arxiv.org/abs/2501.18564)
- Pre-training Auto-regressive Robotic Models with 4D Representations [paper](https://arxiv.org/pdf/2502.13142) [page](https://arm4r.github.io/)
- Flow-based Domain Randomization for Learning and Sequencing Robotic Skills [paper](https://arxiv.org/pdf/2502.01800)
- **EmbodiedBench**: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents [paper](https://arxiv.org/abs/2502.09560) [page](https://embodiedbench.github.io/)
- Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks [paper](https://arxiv.org/abs/2503.01885)
- Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations [paper](https://arxiv.org/abs/2412.14803) [page](https://video-prediction-policy.github.io/)
- **STAR**: Learning Diverse Robot Skill Abstractions through Rotation-Augmented [paper](https://www.arxiv.org/pdf/2506.03863) [page](https://github.com/JiuTian-VL/STAR?tab=readme-ov-file)
## 3D Vision
- Unifying 2D and 3D Vision-Language Understanding [paper](https://arxiv.org/abs/2503.10745) [page](https://univlg.github.io/)
- GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model [paper](https://arxiv.org/abs/2505.04119) [page](https://github.com/zhoujiahuan1991/ICML2025-GAPrompt)

## Dataset
- WOMD-Reasoning: A Large-Scale Dataset for Interaction Reasoning in Driving [paper](https://arxiv.org/abs/2407.04281) [page](https://github.com/yhli123/WOMD-Reasoning)

# RSS2025

- **Unified World Models**: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets [Paper](https://arxiv.org/abs/2504.02792) [Page](https://weirdlabuw.github.io/uwm/)
- **CordViP**: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World [Paper](https://arxiv.org/pdf/2502.08449) [Page](https://aureleopku.github.io/CordViP/)
- **Reactive Diffusion Policy**: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation [Paper](https://arxiv.org/pdf/2503.02881) [Page](https://reactive-diffusion-policy.github.io/)
- Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible Training [Paper](https://arxiv.org/abs/2502.03822)
- **SpatialVLA**: Exploring Spatial Representations for Visual-Language-Action Model [Paper](https://arxiv.org/abs/2501.15830)
- **Sketch-to-Skill**: Bootstrapping Robot Learning with Human Drawn Trajectory Sketches [Paper](https://arxiv.org/abs/2503.11918)
- **NaVILA**: Legged Robot Vision-Language-Action Model for Navigation [Paper](https://arxiv.org/abs/2412.04453) [Page](https://navila-bot.github.io/)
- **ConRFT**: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy [Paper](https://arxiv.org/abs/2502.05450) [Page](https://cccedric.github.io/conrft/)
- **You Only Teach Once**: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations [Paper](https://arxiv.org/abs/2501.14208) [Page](https://hnuzhy.github.io/projects/YOTO/)
- **ASAP**: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills [Paper](https://arxiv.org/abs/2502.01143) [Page](https://agile.human2humanoid.com/)
- **Flying Hand**: End-Effector-Centric Framework for Versatile Aerial Manipulation Teleoperation and Policy Learning [Paper](https://lecar-lab.github.io/flying_hand/static/pdf/flying_hand.pdf) [Page](https://lecar-lab.github.io/flying_hand/)
- **DemoGen**: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning [Paper](https://arxiv.org/abs/2502.16932) [Page](https://demo-generation.github.io/)
- **DOGlove**: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove [Paper](https://arxiv.org/pdf/2502.07730) [Page](https://do-glove.github.io/)
- **RoboSplat**: Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation [Paper](https://arxiv.org/abs/2504.13175) [Page](https://yangsizhe.github.io/robosplat/)
- Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models [Paper](https://arxiv.org/abs/2504.11514)
- **SATA**: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning [Paper](https://arxiv.org/abs/2502.12674) [Video](https://youtu.be/b1cpTq0Rc5w?si=sAd9y5LE2sWynu7v)
- **FACTR**: Force-Attending Curriculum Training for Contact-Rich Policy Learning [Paper](https://arxiv.org/abs/2502.17432) [Page](https://jasonjzliu.com/factr/)
- **RoboVerse**: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning [Paper](https://arxiv.org/abs/2504.18904) [Page](https://roboverseorg.github.io/)
- **STDArm**: Transferring Visuomotor Policies From Static Data Training to Dynamic Robot Manipulation [Paper](https://arxiv.org/abs/2504.18792)

  
# CVPR2025

## Vision-Language-Action Models

- **UniAct**: Universal Actions For Enhanced Embodied Foundation Models [Paper](https://arxiv.org/abs/2501.10105) [Page](https://2toinf.github.io/UniAct/)
- **MoManipVLA**: Transferring Vision-language-action Models for General Mobile Manipulation [Paper](https://arxiv.org/abs/2503.13446) [Page](https://gary3410.github.io/momanipVLA/)
- **CoT-VLA**: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models [Paper](https://cvpr.thecvf.com/virtual/2025/poster/33233)
- **SOLAMI**: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters [Paper](https://arxiv.org/abs/2412.00174) [Page](https://solami-ai.github.io/)
- A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning [Paper](https://arxiv.org/abs/2503.06960) [Page](https://github.com/CVMI-Lab/SlotMIM)
- **Think Small, Act Big**: Primitive Prompt Learning for Lifelong Robot Manipulation
- **Phoenix**: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction [Paper](https://cvpr.thecvf.com/virtual/2025/poster/32789)
- **OmniManip**: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [Paper](https://arxiv.org/abs/2501.03841) [Page](https://omnimanip.github.io/)
- Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation [Paper](https://arxiv.org/abs/2406.14235)
- Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation [Abstract](https://cvpr.thecvf.com/virtual/2025/poster/34522)
- Robotic Visual Instruction
- **RoboGround**: Robot Manipulation with Grounded Vision-Language Priors

## Policies
- **KStar Diffuser**: Spatial-Temporal Graph Diffusion Policy with Kinematics Modeling for Bimanual Robotic Manipulation [Paper](https://arxiv.org/abs/2503.10743)
- **RoboPEPP**: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training [Paper](https://arxiv.org/abs/2411.17662)
- **Lift3D Policy**: Lifting 2D Foundation Models for Robust 3D Robotic Manipulation [Paper](https://arxiv.org/abs/2411.18623) [Page](https://lift3d-web.github.io/)
- **PDFactor**: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation [Abstract](https://cvpr.thecvf.com/virtual/2025/poster/33943)
- **Two by Two**: Learning Cross-Task Pairwise Objects Assembly for Generalizable Robot Manipulation
- **FlowRAM**: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation [Abstract](https://cvpr.thecvf.com/virtual/2025/poster/33579)
- **G3Flow**: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation [Paper](https://arxiv.org/abs/2411.18369) [Page](https://tianxingchen.github.io/G3Flow/)
- **DexHandDiff**: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation [Paper](https://arxiv.org/abs/2411.18562) [Page](https://dexdiffuser.github.io/)
- **Tra-MoE**: Learning Trajectory Prediction Model from Multiple Domains for Adaptive Policy Conditioning [Paper](https://arxiv.org/abs/2411.14519)
- **AffordDP**: Generalizable Diffusion Policy with Transferable Affordance[Paper](https://arxiv.org/abs/2412.03142) [Page](https://afforddp.github.io/)
- **Tra-MoE**: Learning Trajectory Prediction Model from Multiple Domains for Adaptive Policy Conditioning [Paper](https://arxiv.org/abs/2411.14519) [Page](https://github.com/MCG-NJU/Tra-MoE)

## Grasp
- **UniGraspTransformer**: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping [Paper](https://arxiv.org/abs/2412.02699) [Page](https://dexhand.github.io/UniGraspTransformer/)
- **DexGrasp Anything**: Towards Universal Robotic Dexterous Grasping with Physics Awareness [Paper](https://arxiv.org/abs/2503.08257) [Page](https://github.com/4DVLab/DexGrasp-Anything)
- **ZeroGrasp**: Zero-Shot Shape Reconstruction Enabled Robotic Grasping [Paper](https://cvpr.thecvf.com/virtual/2025/poster/32440)

## Humanoid
- Let Humanoid Robots Go Hiking! Integrative Skill Development over Complex Trails [Paper](https://cvpr.thecvf.com/virtual/2025/poster/34565) [Page](https://lego-h-humanoidrobothiking.github.io/)
- **MobileH2R**: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data [Paper](https://arxiv.org/abs/2501.04595)

## 3D Vision
- **3D-MVP**: 3D Multiview Pretraining for Robotic Manipulation [Paper](https://arxiv.org/abs/2406.18158) [Page](https://jasonqsy.github.io/3DMVP/)
- **VidBot**: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation[Paper](https://arxiv.org/abs/2503.07135) [Page](https://hanzhic.github.io/vidbot-project/)
- **Touch2Shape**: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction [Abs](https://cvpr.thecvf.com/virtual/2025/poster/33415)

## Planning and Reasoning
- **RoboBrain**: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete [Paper](https://arxiv.org/abs/2502.21257)
- **PhysVLM**: Enabling Visual Language Models to Understand Robotic Physical Reachability [Paper](https://arxiv.org/abs/2503.08481)
- **RoboSpatial**: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics [Paper](https://arxiv.org/abs/2411.16537)
- **Tartan IMU**: A Light Foundation Model for Inertial Positioning in Robotics [Abstract](https://cvpr.thecvf.com/virtual/2025/poster/33873)
- **Code-as-Monitor**: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection [Paper](https://arxiv.org/abs/2412.04455) [Page](https://zhoues.github.io/Code-as-Monitor/)

## Video

- **TASTE-Rob**: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation [Paper](https://arxiv.org/abs/2503.11423)
- **GraphMimic**: Graph-to-Graphs Generative Modeling from Videos for Policy Learning [Paper](https://cvpr.thecvf.com/virtual/2025/poster/34942)


## Sim2real and Real2sim
- **Prof. Robot**: Differentiable Robot Rendering Without Static and Self-Collisions [Paper](https://arxiv.org/abs/2503.11269) [Page](https://www.qrcat.cn/prof-robot/)
- **AutoURDF**: Unsupervised Robot Modeling from Point Cloud Frames Using Cluster Registration [Paper](https://arxiv.org/abs/2412.05507) [Page](https://github.com/jl6017/AutoURDF)

## Benchmark and Dataset
- **RoboTwin**: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)[Paper](https://arxiv.org/abs/2409.02920) [Page](https://robotwin-benchmark.github.io/early-version/)
- Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision [Paper](https://arxiv.org/abs/2411.18025)
- **RoboSense**: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments [Paper](https://arxiv.org/abs/2408.15503) [Page](https://github.com/suhaisheng/RoboSense)

# ICLR2025

## Vision-Language-Action Models

- **LLaRA**: Supercharging Robot Learning Data for Vision-Language Policy [Paper](https://arxiv.org/abs/2406.20095) [Page](https://github.com/LostXine/LLaRA)
- **VLAS**: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation [Paper](https://arxiv.org/abs/2502.13508) [Page](https://github.com/whichwhichgone/VLAS)
- **TraceVLA**: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies [Paper](https://arxiv.org/abs/2412.10345) [Page](https://tracevla.github.io/)
- **Robots Pre-train Robots**: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets [Paper](https://arxiv.org/abs/2410.22325) [Page](https://robots-pretrain-robots.github.io/)
- **PIDM**: Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation [Paper](https://arxiv.org/abs/2412.15109) [Page](https://nimolty.github.io/Seer/)

## Policies

- **GravMAD**: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation [Paper](https://arxiv.org/abs/2409.20154) [Page](https://gravmad.github.io/)
- **ReViWo**: Learning View-invariant World Models for Visual Robotic Manipulation [Paper](https://openreview.net/forum?id=vJwjWyt4Ed) [zhihu](https://zhuanlan.zhihu.com/p/26181243574)
- **HAMSTER**: Hierarchical Action Models For Open-World Robot Manipulation [Paper](https://arxiv.org/abs/2502.05485) [Page](https://hamster-robot.github.io/)
- **BadRobot**: Jailbreaking Embodied LLMs in the Physical World [Paper](https://arxiv.org/abs/2407.20242) [Page](https://embodied-llms-safety.github.io/)
- **STRAP**: Robot Sub-Trajectory Retrieval for Augmented Policy Learning [Paper](https://arxiv.org/abs/2412.15182) [Page](https://weirdlabuw.github.io/strap/)
- **SRSA**: Skill Retrieval and Adaptation for Robotic Assembly Tasks [Paper](https://arxiv.org/abs/2503.04538) [Page](https://srsa2024.github.io/)
- Data Scaling Laws in Imitation Learning for Robotic Manipulation [Paper](https://arxiv.org/abs/2410.18647) [Page](https://data-scaling-laws.github.io/)
- **Stem-OB**: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion [Paper](https://arxiv.org/abs/2411.04919) [Page](https://hukz18.github.io/Stem-Ob/)

## 3D Vision
- **Dream to Manipulate**: Compositional World Models Empowering Robot Imitation Learning with Imagination [Paper](https://arxiv.org/abs/2412.14957) [Page](https://leobarcellona.github.io/DreamToManipulate/)
- **SPA***: 3D Spatial-Awareness Enables Effective Embodied Representation [Paper](https://arxiv.org/abs/2410.08208) [Page](https://haoyizhu.github.io/spa/)

## Planning and Reasoning
- **LASeR**: Towards Diversified and Generalizable Robot Design with Large Language Models [Paper](https://openreview.net/forum?id=7mlvOHL6qJ) [Page](https://github.com/WoodySJR/LASeR)
- Physics-informed Temporal Difference Metric Learning for Robot Motion Planning [Paper](https://openreview.net/forum?id=TOiageVNru) [Page](https://github.com/ruiqini/ntrl-demo)
- **AHA**: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation [Paper](https://arxiv.org/abs/2410.00371) [Page](https://aha-vlm.github.io/)
- **EMOS**: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents[Paper](https://arxiv.org/abs/2410.22662) [Page](https://arxiv.org/abs/2410.22662)
- **VisualPredicator**: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning [Paper](https://arxiv.org/abs/2410.23156) [Page](https://arxiv.org/abs/2410.23156)
- **DenseMatcher**: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo [Paper](https://arxiv.org/abs/2412.05268) [Page](https://tea-lab.github.io/DenseMatcher/)
- 6D Object Pose Tracking in Internet Videos for Robotic Manipulation [Paper](https://arxiv.org/abs/2503.10307) [Page](https://ponimatkin.github.io/freepose/)

## Planning and Reasoning
- Multi-Robot Motion Planning with Diffusion Models [Paper](https://arxiv.org/abs/2410.03072) [Page](https://github.com/yoraish/mmd)

## Video
- **GEVRM**: Goal-Expressive Video Generation Model For Robust Visual Manipulation [Paper](https://arxiv.org/abs/2502.09268)

## Sim2real and Real2sim
- **ReGen**: Generative Robot Simulation via Inverse Design [Paper](https://openreview.net/forum?id=EbCUbPZjM1) [Page](https://regen-sim.github.io/)
  
# ICRA2025
- **MoRE**: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models [Paper](https://arxiv.org/abs/2503.08007)
- **QUART-Online**: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning [Paper](https://arxiv.org/abs/2412.15576) [Page](https://quart-online.github.io/)
- **SpatialBot**: Precise Spatial Understanding with Vision Language Models [Paper](https://arxiv.org/pdf/2406.13642) [Page](https://github.com/BAAI-DCAI/SpatialBot)


